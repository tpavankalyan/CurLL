
🔍 Loading model and tokenizer for 'HuggingFaceTB/SmolLM2-360M'...

🔢 Parameter Breakdown
----------------------
Embedding parameters:     47,185,920
Non-embedding parameters: 314,635,200
Total parameters:         361,821,120

⚙️ Model Config Info
--------------------
Model class: LlamaModel
Architectures: ['LlamaForCausalLM']
Hidden size: 960
Intermediate size: 2560
Num hidden layers: 32
Num attention heads: 15
Max position embeddings: 8192
Tied embeddings: True
Is encoder: False
Is decoder-only: False

🔤 Tokenizer Info
-----------------
Tokenizer class: GPT2TokenizerFast
Vocab size: 49,152
Special tokens:
  pad_token   : None
  bos_token   : '<|endoftext|>'
  eos_token   : '<|endoftext|>'
  unk_token   : '<|endoftext|>'
  cls_token   : None
  sep_token   : None
  mask_token  : None
