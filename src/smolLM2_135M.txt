
🔍 Loading model and tokenizer for 'HuggingFaceTB/SmolLM2-135M'...

🔢 Parameter Breakdown
----------------------
Embedding parameters:     28,311,552
Non-embedding parameters: 106,203,456
Total parameters:         134,515,008

⚙️ Model Config Info
--------------------
Model class: LlamaModel
Architectures: ['LlamaForCausalLM']
Hidden size: 576
Intermediate size: 1536
Num hidden layers: 30
Num attention heads: 9
Max position embeddings: 8192
Tied embeddings: True
Is encoder: False
Is decoder-only: False

🔤 Tokenizer Info
-----------------
Tokenizer class: GPT2TokenizerFast
Vocab size: 49,152
Special tokens:
  pad_token   : None
  bos_token   : '<|endoftext|>'
  eos_token   : '<|endoftext|>'
  unk_token   : '<|endoftext|>'
  cls_token   : None
  sep_token   : None
  mask_token  : None
